{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from shapely.validation import make_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import CENSUS_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>POLYGON ((610569.054 4822163.339, 611569.054 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>POLYGON ((611569.054 4822163.339, 612569.054 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>POLYGON ((612569.054 4823163.339, 613569.054 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>POLYGON ((609569.054 4823163.339, 610569.054 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>POLYGON ((610569.054 4823163.339, 611569.054 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               geometry\n",
       "1288  POLYGON ((610569.054 4822163.339, 611569.054 4...\n",
       "1349  POLYGON ((611569.054 4822163.339, 612569.054 4...\n",
       "1411  POLYGON ((612569.054 4823163.339, 613569.054 4...\n",
       "1228  POLYGON ((609569.054 4823163.339, 610569.054 4...\n",
       "1289  POLYGON ((610569.054 4823163.339, 611569.054 4..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GTA municipalities boundary\n",
    "gta = gpd.read_file(\"../data/geo/regions/TMUN.gpkg\")\n",
    "\n",
    "# Ensure geometry validity\n",
    "gta[\"geometry\"] = gta[\"geometry\"].apply(make_valid)\n",
    "\n",
    "# --- 1. Reproject to a projected CRS (meters) ---\n",
    "# Toronto is in UTM Zone 17N\n",
    "CRS_METERS = \"EPSG:32617\"\n",
    "gta_proj = gta.to_crs(CRS_METERS)\n",
    "\n",
    "# Dissolve to single geometry for intersection tests\n",
    "gta_boundary = gta_proj.dissolve()\n",
    "\n",
    "# --- 2. Create bounding box for grid ---\n",
    "minx, miny, maxx, maxy = gta_proj.total_bounds\n",
    "\n",
    "# Set grid resolution\n",
    "cell_size = 1000  # meters\n",
    "\n",
    "# Generate grid coordinates\n",
    "x_coords = np.arange(minx, maxx + cell_size, cell_size)\n",
    "y_coords = np.arange(miny, maxy + cell_size, cell_size)\n",
    "\n",
    "# Generate polygons for each 1 km square\n",
    "grid_polys = []\n",
    "for x in x_coords:\n",
    "    for y in y_coords:\n",
    "        grid_polys.append(\n",
    "            Polygon([\n",
    "                (x, y),\n",
    "                (x + cell_size, y),\n",
    "                (x + cell_size, y + cell_size),\n",
    "                (x, y + cell_size)\n",
    "            ])\n",
    "        )\n",
    "\n",
    "grid = gpd.GeoDataFrame({\"geometry\": grid_polys}, crs=CRS_METERS)\n",
    "\n",
    "# --- 3. Keep only full squares that intersect the TMUN boundary ---\n",
    "# Build a spatial index for performance\n",
    "sindex = grid.sindex\n",
    "\n",
    "possible_matches_idx = list(sindex.intersection(gta_boundary.geometry.total_bounds))\n",
    "possible_matches = grid.iloc[possible_matches_idx]\n",
    "\n",
    "intersects_mask = possible_matches.intersects(gta_boundary.geometry.iloc[0])\n",
    "\n",
    "grid_intersecting = possible_matches[intersects_mask].copy()\n",
    "\n",
    "# --- 4. Save as GeoPackage ---\n",
    "OUT_PATH = \"../data/geo/regions/TMUN_squares.gpkg\"\n",
    "grid_intersecting.to_file(OUT_PATH, driver=\"GPKG\")\n",
    "\n",
    "grid_intersecting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpolating year 1971...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "year 1971 tracts: 100%|██████████| 422/422 [00:04<00:00, 89.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved interpolated squares for 1971 -> ../data/language/1971/num_speakers_squares_1971.gpkg\n",
      "\n",
      "Interpolating year 1996...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "year 1996 tracts: 100%|██████████| 713/713 [00:09<00:00, 72.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved interpolated squares for 1996 -> ../data/language/1996/num_speakers_squares_1996.gpkg\n",
      "\n",
      "Interpolating year 2021...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "year 2021 tracts: 100%|██████████| 1049/1049 [00:27<00:00, 38.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved interpolated squares for 2021 -> ../data/language/2021/num_speakers_squares_2021.gpkg\n",
      "\n",
      "All years complete.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Cell 4 — Areal interpolation (tract -> 1km squares)\n",
    "# ---------------------------\n",
    "# Where the squares live (created in Cell 3)\n",
    "SQUARES_PATH = \"../data/geo/regions/TMUN_squares.gpkg\"\n",
    "\n",
    "# Output file pattern (one file per year)\n",
    "OUT_PATTERN = \"../data/language/{year}/num_speakers_squares_{year}.gpkg\"\n",
    "\n",
    "# Ensure output directories exist when we write later\n",
    "def _ensure_out_dir(path_str):\n",
    "    p = Path(path_str).parent\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load squares once (we will reuse for all years)\n",
    "squares = gpd.read_file(SQUARES_PATH)\n",
    "\n",
    "# Ensure squares are projected in meters for area calculations\n",
    "CRS_METERS = \"EPSG:32617\"\n",
    "if squares.crs is None:\n",
    "    raise ValueError(f\"{SQUARES_PATH} has no CRS; please ensure it is projected (EPSG:32617 recommended).\")\n",
    "squares = squares.to_crs(CRS_METERS)\n",
    "squares = squares.reset_index(drop=True)  # ensure clean integer index\n",
    "\n",
    "# Build a spatial index for squares for fast intersection queries\n",
    "squares_sindex = squares.sindex\n",
    "\n",
    "# Main loop over census years\n",
    "for year in CENSUS_25:\n",
    "    print(f\"\\nInterpolating year {year}...\")\n",
    "\n",
    "    # Load the tract-level counts for this year\n",
    "    tract_path = f\"../data/language/{year}/num_speakers_tmun_{year}.gpkg\"\n",
    "    tracts = gpd.read_file(tract_path)\n",
    "\n",
    "    # Ensure projected CRS for area computations\n",
    "    tracts = tracts.to_crs(CRS_METERS)\n",
    "\n",
    "    # Identify the columns we want to interpolate:\n",
    "    # all columns that start with \"num_\" (including num_tot and num_not_eng)\n",
    "    num_cols = [c for c in tracts.columns if c.startswith(\"num_\") and c != \"geometry\"]\n",
    "    if \"num_tot\" not in num_cols:\n",
    "        # Safety check, include if present\n",
    "        if \"num_tot\" in tracts.columns:\n",
    "            num_cols.append(\"num_tot\")\n",
    "\n",
    "    # Prepare result GeoDataFrame: copy squares geometry and zeroed numeric columns\n",
    "    result = squares.copy()\n",
    "    for c in num_cols:\n",
    "        result[c] = 0.0\n",
    "\n",
    "    # For performance, convert numeric tract columns to numpy (we will access them per-tract)\n",
    "    # but keep them available in the tracts GeoDataFrame for clarity.\n",
    "    tracts[num_cols] = tracts[num_cols].astype(float)\n",
    "\n",
    "    # Iterate over tracts and distribute counts to intersecting squares\n",
    "    # Using tqdm for progress visibility\n",
    "    for idx, tract in tqdm(tracts.iterrows(), total=len(tracts), desc=f\"year {year} tracts\"):\n",
    "        tract_geom = tract.geometry\n",
    "        if tract_geom is None or tract_geom.is_empty:\n",
    "            continue\n",
    "\n",
    "        tract_area = tract_geom.area\n",
    "        if tract_area == 0 or np.isnan(tract_area):\n",
    "            continue\n",
    "\n",
    "        # candidate square indices whose bounding boxes intersect the tract bbox\n",
    "        possible_idx = list(squares_sindex.intersection(tract_geom.bounds))\n",
    "        if not possible_idx:\n",
    "            continue\n",
    "\n",
    "        candidates = result.iloc[possible_idx]  # note: result has same index as squares\n",
    "        # compute actual intersections (this returns a GeoSeries)\n",
    "        intersections = candidates.geometry.intersection(tract_geom)\n",
    "\n",
    "        # compute intersection areas\n",
    "        inter_areas = intersections.area\n",
    "\n",
    "        # mask where area > 0\n",
    "        mask = inter_areas > 0\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        # indices (in result GeoDataFrame) that receive contributions\n",
    "        recv_idx = candidates.index[mask]\n",
    "\n",
    "        # compute ratios (proportion of tract area that falls in each square)\n",
    "        ratios = inter_areas[mask] / tract_area  # pandas Series indexed by recv_idx\n",
    "\n",
    "        # For each numeric column, add the apportioned count to the corresponding square rows\n",
    "        # We vectorize per-column by multiplying ratios (aligned by index)\n",
    "        for col in num_cols:\n",
    "            tract_value = tract[col] if pd.notnull(tract[col]) else 0.0\n",
    "            if tract_value == 0:\n",
    "                # nothing to add for this column\n",
    "                continue\n",
    "\n",
    "            # apportioned counts per receiving square (Series aligned to recv_idx)\n",
    "            apportioned = ratios * float(tract_value)\n",
    "\n",
    "            # add apportioned values into result dataframe (aligning on index)\n",
    "            # use .loc to write values for these indices\n",
    "            result.loc[recv_idx, col] += apportioned.values\n",
    "\n",
    "    # Optional: round counts to, say, 2 decimals (population counts can remain floats but you can round)\n",
    "    # Here we'll keep floats but fill any tiny negatives (due to numerical issues) with 0\n",
    "    for c in num_cols:\n",
    "        result[c] = result[c].clip(lower=0.0)\n",
    "\n",
    "    # Save the interpolated squares for this year\n",
    "    out_path = OUT_PATTERN.format(year=year)\n",
    "    _ensure_out_dir(out_path)\n",
    "    result.to_file(out_path, driver=\"GPKG\")\n",
    "    print(f\"Saved interpolated squares for {year} -> {out_path}\")\n",
    "\n",
    "print(\"\\nAll years complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1971…\n",
      "✓ Finished 1971\n",
      "Processing 1996…\n",
      "✓ Finished 1996\n",
      "Processing 2021…\n",
      "✓ Finished 2021\n"
     ]
    }
   ],
   "source": [
    "# CELL 5 - compute pct files, round num files, create centroid versions\n",
    "IN_PATTERN  = \"../data/language/{year}/num_speakers_squares_{year}.gpkg\"\n",
    "PCT_PATTERN = \"../data/language/{year}/pct_speakers_squares_{year}.gpkg\"\n",
    "\n",
    "CENT_NUM_PATTERN = \"../data/language/{year}/num_speakers_centroid_{year}.gpkg\"\n",
    "CENT_PCT_PATTERN = \"../data/language/{year}/pct_speakers_centroid_{year}.gpkg\"\n",
    "\n",
    "for year in CENSUS_25:\n",
    "    print(f\"Processing {year}…\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Load NUM dataset\n",
    "    # ------------------------------\n",
    "    num_path = IN_PATTERN.format(year=year)\n",
    "    gdf_num = gpd.read_file(num_path)\n",
    "\n",
    "    # Identify all language count columns (start with 'num_' but exclude num_tot)\n",
    "    lang_cols = [c for c in gdf_num.columns if c.startswith(\"num_\") and c != \"num_tot\"]\n",
    "\n",
    "    # ------------------------------\n",
    "    # Compute percentages\n",
    "    # ------------------------------\n",
    "    gdf_pct = gdf_num.copy()\n",
    "\n",
    "    for col in lang_cols:\n",
    "        lang = col.replace(\"num_\", \"\")\n",
    "        gdf_pct[f\"pct_{lang}\"] = (gdf_pct[col] / gdf_pct[\"num_tot\"] * 100).round(2)\n",
    "\n",
    "    # Keep num_tot (rounded)\n",
    "    gdf_pct[\"num_tot\"] = gdf_pct[\"num_tot\"].round(2)\n",
    "\n",
    "    # Drop original num language columns for pct file\n",
    "    gdf_pct = gdf_pct.drop(columns=lang_cols)\n",
    "\n",
    "    # Save pct file\n",
    "    pct_path = PCT_PATTERN.format(year=year)\n",
    "    gdf_pct.to_file(pct_path, driver=\"GPKG\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Round all numeric values in NUM file\n",
    "    # ------------------------------\n",
    "    for col in lang_cols + [\"num_tot\"]:\n",
    "        gdf_num[col] = gdf_num[col].round(2)\n",
    "\n",
    "    gdf_num.to_file(num_path, driver=\"GPKG\")  # overwrite with rounded values\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create centroid versions\n",
    "    # ------------------------------\n",
    "    gdf_num_cent = gdf_num.copy()\n",
    "    gdf_num_cent[\"geometry\"] = gdf_num_cent.centroid\n",
    "    gdf_num_cent.to_file(CENT_NUM_PATTERN.format(year=year), driver=\"GPKG\")\n",
    "\n",
    "    gdf_pct_cent = gdf_pct.copy()\n",
    "    gdf_pct_cent[\"geometry\"] = gdf_pct_cent.centroid\n",
    "    gdf_pct_cent.to_file(CENT_PCT_PATTERN.format(year=year), driver=\"GPKG\")\n",
    "\n",
    "    print(f\"✓ Finished {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 1971...\n",
      "✓ Saved ../data/language/1971/num_speakers_centroid_1971.json (2255 rows)\n",
      "Processing year 1996...\n",
      "✓ Saved ../data/language/1996/num_speakers_centroid_1996.json (2255 rows)\n",
      "Processing year 2021...\n",
      "✓ Saved ../data/language/2021/num_speakers_centroid_2021.json (2255 rows)\n"
     ]
    }
   ],
   "source": [
    "# === Cell N: Convert centroid GPKGs into frontend-ready JSON ===\n",
    "for year in CENSUS_25:\n",
    "    in_path = f\"../data/language/{year}/num_speakers_centroid_{year}.gpkg\"\n",
    "    out_path = f\"../data/language/{year}/num_speakers_centroid_{year}.json\"\n",
    "\n",
    "    print(f\"Processing year {year}...\")\n",
    "\n",
    "    # Load\n",
    "    gdf = gpd.read_file(in_path)\n",
    "\n",
    "    # Ensure CRS is WGS84\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(4326)\n",
    "\n",
    "    # Extract centroid coords\n",
    "    # (geometry *should* already be a point but we recompute to be safe)\n",
    "    gdf[\"x\"] = gdf.geometry.x\n",
    "    gdf[\"y\"] = gdf.geometry.y\n",
    "\n",
    "    # Keep numeric columns beginning with \"num_\"\n",
    "    num_cols = [c for c in gdf.columns if c.startswith(\"num_\")]\n",
    "\n",
    "    # Build clean DataFrame for export\n",
    "    df = gdf[[\"x\", \"y\"] + num_cols].copy()\n",
    "\n",
    "    # Convert to records for JSON output\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "\n",
    "    # Save JSON\n",
    "    (\n",
    "        pd.DataFrame(records)\n",
    "        .to_json(out_path, orient=\"records\", indent=2)\n",
    "    )\n",
    "\n",
    "    print(f\"✓ Saved {out_path} ({len(df)} rows)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
